{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carousell Scraping V1.0 by Billy Cao\n",
      "Running on Python 3.9.9 (tags/v3.9.9:ccb0e6a, Nov 15 2021, 18:08:50) [MSC v.1929 64 bit (AMD64)], Selenium 4.1.0, BeautifulSoup 4.10.0\n",
      "Chrome Web Driver loaded. Version: 97.0.4692.71\n",
      "\n",
      "Retrieving search results on apple...\n",
      "All results loaded. Total: 1 pages.\n",
      "Detected item_divs class: D_wt D_tm D_tr\n",
      "Found 19 listings. Parsing...\n",
      "<div class=\"D_t_\"><div class=\"D_tu\" style=\"background-color: rgb(240, 241, 241);\"><img alt=\"\" class=\"D_lu D_lr D_tv\" src=\"https://sl3-cdn.karousell.com/components/tag_icons/protection@xxxhdpi.png\" title=\"\"/><p class=\"D_hg D_fK D_hh D_hk D_hm D_hp D_hr D_hc\">Protection</p></div><div class=\"D_rm D_tK D_rr D_rn\"><p class=\"D_hg D_fN D_hh D_hk D_hn D_hp D_hr D_he\">Spotlight</p></div><div class=\"D_yi D_yl\"><img alt=\"Buy in  new,used,spoilt Macbooks &amp; laptops ,iMac \" class=\"D_lu D_lr D_yp\" src=\"https://media.karousell.com/media/photos/products/2020/10/7/buy_in_newusedspoilt_laptops___1602050065_479a4837_progressive_thumbnail.jpg\" title=\"Buy in  new,used,spoilt Macbooks &amp; laptops ,iMac \"/></div></div>\n",
      "<div class=\"D_t_\"><div class=\"D_tu\" style=\"background-color: rgb(240, 241, 241);\"><img alt=\"\" class=\"D_lu D_lr D_tv\" src=\"https://sl3-cdn.karousell.com/components/tag_icons/protection@xxxhdpi.png\" title=\"\"/><p class=\"D_hg D_fK D_hh D_hk D_hm D_hp D_hr D_hc\">Protection</p></div><div class=\"D_rm D_tK D_rr D_rn\"><p class=\"D_hg D_fN D_hh D_hk D_hn D_hp D_hr D_he\">Spotlight</p></div><div class=\"D_yi D_yl\"><img alt=\"Buy all laptop,macbook iMac used or spoilt faulty and new\" class=\"D_lu D_lr D_yp\" src=\"https://media.karousell.com/media/photos/products/2020/10/13/we_buy_in_all_macbookslaptopsi_1602555168_81e4f250_progressive_thumbnail.jpg\" title=\"Buy all laptop,macbook iMac used or spoilt faulty and new\"/></div></div>\n",
      "<div class=\"D_t_\"><div class=\"D_yi D_yl\"><img alt=\"Apple Watch Series3 (not working)\" class=\"D_lu D_lr D_yp\" src=\"https://media.karousell.com/media/photos/products/2022/1/15/apple_watch_series3_not_workin_1642233272_d6cf4414_progressive_thumbnail.jpg\" title=\"Apple Watch Series3 (not working)\"/></div></div>\n",
      "<div class=\"D_t_\"><div class=\"D_yi D_yl\"><img alt=\"Apple Watch Series 7 45mm\" class=\"D_lu D_lr D_yp\" src=\"https://media.karousell.com/media/photos/products/2022/1/15/apple_watch_series_7_45mm_1642233664_6d1b905a_progressive_thumbnail.jpg\" title=\"Apple Watch Series 7 45mm\"/></div></div>\n",
      "<div class=\"D_t_\"><div class=\"D_yi D_yl\"><img alt=\"Apple Watch 6 Titanium Space Black 44mm AppleCare+ 2023\" class=\"D_lu D_lr D_yp\" src=\"https://media.karousell.com/media/photos/products/2022/1/9/apple_watch_6_titanium_space_b_1641699217_773c4a04_progressive_thumbnail\" title=\"Apple Watch 6 Titanium Space Black 44mm AppleCare+ 2023\"/></div></div>\n",
      "<div class=\"D_t_\"><div class=\"D_yi D_yl\"><img alt=\"Apple Watch Series 6 Black\" class=\"D_lu D_lr D_yp\" src=\"https://media.karousell.com/media/photos/products/2022/1/15/apple_watch_series_6_black_1642230611_aa48f72e_progressive_thumbnail.jpg\" title=\"Apple Watch Series 6 Black\"/></div></div>\n",
      "<div class=\"D_t_\"><div class=\"D_tu\" style=\"background-color: rgb(240, 241, 241);\"><img alt=\"\" class=\"D_lu D_lr D_tv\" src=\"https://sl3-cdn.karousell.com/components/tag_icons/protection@xxxhdpi.png\" title=\"\"/><p class=\"D_hg D_fK D_hh D_hk D_hm D_hp D_hr D_hc\">Protection</p></div><div class=\"D_yi D_yl\"><img alt=\"Apple Watch SE (40mm GPS)\" class=\"D_lu D_lr D_yp\" src=\"https://media.karousell.com/media/photos/products/2022/1/15/apple_watch_se_40mm_gps_1642232429_7c0091fd_progressive_thumbnail.jpg\" title=\"Apple Watch SE (40mm GPS)\"/></div></div>\n",
      "<div class=\"D_t_\"><div><div class=\"D_gY D_yj\"></div></div></div>\n",
      "<div class=\"D_t_\"><div class=\"D_tu\" style=\"background-color: rgb(240, 241, 241);\"><img alt=\"\" class=\"D_lu D_lr D_tv\" src=\"https://sl3-cdn.karousell.com/components/tag_icons/protection@xxxhdpi.png\" title=\"\"/><p class=\"D_hg D_fK D_hh D_hk D_hm D_hp D_hr D_hc\">Protection</p></div><div class=\"D_rm D_tK D_rr D_rn\"><p class=\"D_hg D_fN D_hh D_hk D_hn D_hp D_hr D_he\">Spotlight</p></div><div><div class=\"D_gY D_yj\"></div></div></div>\n",
      "<div class=\"D_t_\"><div><div class=\"D_gY D_yj\"></div></div></div>\n",
      "<div class=\"D_t_\"><div><div class=\"D_gY D_yj\"></div></div></div>\n",
      "<div class=\"D_t_\"><div><div class=\"D_gY D_yj\"></div></div></div>\n",
      "<div class=\"D_t_\"><div><div class=\"D_gY D_yj\"></div></div></div>\n",
      "<div class=\"D_t_\"><div class=\"D_tu\" style=\"background-color: rgb(240, 241, 241);\"><img alt=\"\" class=\"D_lu D_lr D_tv\" src=\"https://sl3-cdn.karousell.com/components/tag_icons/protection@xxxhdpi.png\" title=\"\"/><p class=\"D_hg D_fK D_hh D_hk D_hm D_hp D_hr D_hc\">Protection</p></div><div class=\"D_rm D_tK D_rr D_rn\"><p class=\"D_hg D_fN D_hh D_hk D_hn D_hp D_hr D_he\">Spotlight</p></div><div><div class=\"D_gY D_yj\"></div></div></div>\n",
      "<div class=\"D_t_\"><div><div class=\"D_gY D_yj\"></div></div></div>\n",
      "<div class=\"D_t_\"><div class=\"D_tu\" style=\"background-color: rgb(240, 241, 241);\"><img alt=\"\" class=\"D_lu D_lr D_tv\" src=\"https://sl3-cdn.karousell.com/components/tag_icons/protection@xxxhdpi.png\" title=\"\"/><p class=\"D_hg D_fK D_hh D_hk D_hm D_hp D_hr D_hc\">Protection</p></div><div><div class=\"D_gY D_yj\"></div></div></div>\n",
      "<div class=\"D_t_\"><div><div class=\"D_gY D_yj\"></div></div></div>\n",
      "<div class=\"D_t_\"><div><div class=\"D_gY D_yj\"></div></div></div>\n",
      "<div class=\"D_t_\"><div class=\"D_tu\" style=\"background-color: rgb(240, 241, 241);\"><img alt=\"\" class=\"D_lu D_lr D_tv\" src=\"https://sl3-cdn.karousell.com/components/tag_icons/protection@xxxhdpi.png\" title=\"\"/><p class=\"D_hg D_fK D_hh D_hk D_hm D_hp D_hr D_hc\">Protection</p></div><div class=\"D_rm D_tK D_rr D_rn\"><p class=\"D_hg D_fN D_hh D_hk D_hn D_hp D_hr D_he\">Spotlight</p></div><div><div class=\"D_gY D_yj\"></div></div></div>\n",
      "Parse success using mode 1! Sample item parsed:\n",
      "{'condition': 'Used',\n",
      " 'img': 'https://media.karousell.com/media/photos/products/2020/10/7/buy_in_newusedspoilt_laptops___1602050065_479a4837_progressive_thumbnail.jpg',\n",
      " 'item_name': 'Buy in  new,used,spoilt Macbooks & laptops ,iMac ',\n",
      " 'item_url': 'https://sg.carousell.com/p/buy-in-new-used-spoilt-macbooks-laptops-imac-1031253074/?t-id=SbgicjzK7N_1642233959665&t-referrer_browse_type=search_results&t-referrer_request_id=gmhINDOgLZFzdtgj&t-referrer_search_query=apple&t-referrer_sort_by=popular&t-tap_index=0',\n",
      " 'likes': '2k',\n",
      " 'price': '2900',\n",
      " 'seller_name': 'zhapkua',\n",
      " 'seller_url': 'https://sg.carousell.com/u/zhapkua/?t-id=SbgicjzK7N_1642233959665&t-source=search_results&t-source_query=apple',\n",
      " 'time_posted': '1 year ago'}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[1;32mIn [40]\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m    116\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mParse success using mode \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparse_mode\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m! Sample item parsed:\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    117\u001B[0m pprint(items_list[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m--> 118\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[43mitems_list\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    119\u001B[0m \u001B[38;5;28mprint\u001B[39m(df\u001B[38;5;241m.\u001B[39mdescribe())\n\u001B[0;32m    120\u001B[0m \u001B[38;5;28mprint\u001B[39m(df)\n",
      "File \u001B[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\pandas\\core\\frame.py:694\u001B[0m, in \u001B[0;36mDataFrame.__init__\u001B[1;34m(self, data, index, columns, dtype, copy)\u001B[0m\n\u001B[0;32m    689\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m columns \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    690\u001B[0m         \u001B[38;5;66;03m# error: Argument 1 to \"ensure_index\" has incompatible type\u001B[39;00m\n\u001B[0;32m    691\u001B[0m         \u001B[38;5;66;03m# \"Collection[Any]\"; expected \"Union[Union[Union[ExtensionArray,\u001B[39;00m\n\u001B[0;32m    692\u001B[0m         \u001B[38;5;66;03m# ndarray], Index, Series], Sequence[Any]]\"\u001B[39;00m\n\u001B[0;32m    693\u001B[0m         columns \u001B[38;5;241m=\u001B[39m ensure_index(columns)  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m--> 694\u001B[0m     arrays, columns, index \u001B[38;5;241m=\u001B[39m \u001B[43mnested_data_to_arrays\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    695\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001B[39;49;00m\n\u001B[0;32m    696\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001B[39;49;00m\n\u001B[0;32m    697\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    698\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    699\u001B[0m \u001B[43m        \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[0;32m    700\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    701\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    702\u001B[0m     mgr \u001B[38;5;241m=\u001B[39m arrays_to_mgr(\n\u001B[0;32m    703\u001B[0m         arrays,\n\u001B[0;32m    704\u001B[0m         columns,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    708\u001B[0m         typ\u001B[38;5;241m=\u001B[39mmanager,\n\u001B[0;32m    709\u001B[0m     )\n\u001B[0;32m    710\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\pandas\\core\\internals\\construction.py:483\u001B[0m, in \u001B[0;36mnested_data_to_arrays\u001B[1;34m(data, columns, index, dtype)\u001B[0m\n\u001B[0;32m    480\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_named_tuple(data[\u001B[38;5;241m0\u001B[39m]) \u001B[38;5;129;01mand\u001B[39;00m columns \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    481\u001B[0m     columns \u001B[38;5;241m=\u001B[39m ensure_index(data[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39m_fields)\n\u001B[1;32m--> 483\u001B[0m arrays, columns \u001B[38;5;241m=\u001B[39m \u001B[43mto_arrays\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    484\u001B[0m columns \u001B[38;5;241m=\u001B[39m ensure_index(columns)\n\u001B[0;32m    486\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m index \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\pandas\\core\\internals\\construction.py:799\u001B[0m, in \u001B[0;36mto_arrays\u001B[1;34m(data, columns, dtype)\u001B[0m\n\u001B[0;32m    797\u001B[0m     arr \u001B[38;5;241m=\u001B[39m _list_to_arrays(data)\n\u001B[0;32m    798\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data[\u001B[38;5;241m0\u001B[39m], abc\u001B[38;5;241m.\u001B[39mMapping):\n\u001B[1;32m--> 799\u001B[0m     arr, columns \u001B[38;5;241m=\u001B[39m \u001B[43m_list_of_dict_to_arrays\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    800\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data[\u001B[38;5;241m0\u001B[39m], ABCSeries):\n\u001B[0;32m    801\u001B[0m     arr, columns \u001B[38;5;241m=\u001B[39m _list_of_series_to_arrays(data, columns)\n",
      "File \u001B[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\pandas\\core\\internals\\construction.py:884\u001B[0m, in \u001B[0;36m_list_of_dict_to_arrays\u001B[1;34m(data, columns)\u001B[0m\n\u001B[0;32m    882\u001B[0m     gen \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mlist\u001B[39m(x\u001B[38;5;241m.\u001B[39mkeys()) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m data)\n\u001B[0;32m    883\u001B[0m     sort \u001B[38;5;241m=\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(d, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m data)\n\u001B[1;32m--> 884\u001B[0m     pre_cols \u001B[38;5;241m=\u001B[39m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfast_unique_multiple_list_gen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgen\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msort\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msort\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    885\u001B[0m     columns \u001B[38;5;241m=\u001B[39m ensure_index(pre_cols)\n\u001B[0;32m    887\u001B[0m \u001B[38;5;66;03m# assure that they are of the base dict class and not of derived\u001B[39;00m\n\u001B[0;32m    888\u001B[0m \u001B[38;5;66;03m# classes\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\pandas\\_libs\\lib.pyx:400\u001B[0m, in \u001B[0;36mpandas._libs.lib.fast_unique_multiple_list_gen\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\pandas\\core\\internals\\construction.py:882\u001B[0m, in \u001B[0;36m<genexpr>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    862\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    863\u001B[0m \u001B[38;5;124;03mConvert list of dicts to numpy arrays\u001B[39;00m\n\u001B[0;32m    864\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    879\u001B[0m \u001B[38;5;124;03mcolumns : Index\u001B[39;00m\n\u001B[0;32m    880\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    881\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m columns \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 882\u001B[0m     gen \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mlist\u001B[39m(\u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkeys\u001B[49m()) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m data)\n\u001B[0;32m    883\u001B[0m     sort \u001B[38;5;241m=\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(d, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m data)\n\u001B[0;32m    884\u001B[0m     pre_cols \u001B[38;5;241m=\u001B[39m lib\u001B[38;5;241m.\u001B[39mfast_unique_multiple_list_gen(gen, sort\u001B[38;5;241m=\u001B[39msort)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import time\n",
    "import re\n",
    "import urllib\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "print(f'Carousell Scraping V1.0 by Billy Cao\\nRunning on Python {sys.version}, Selenium {selenium.__version__}, BeautifulSoup {bs4.__version__}')\n",
    "\n",
    "def request_page(url):\n",
    "    \"\"\" Returns BeautifulSoup4 Objects (soup)\"\"\"\n",
    "    driver.get(url)\n",
    "    page = 1\n",
    "    timeout = 5\n",
    "    if page_limit:\n",
    "        while page < page_limit:\n",
    "            try:\n",
    "                next_page_btn = WebDriverWait(driver, timeout).until(EC.presence_of_element_located((By.XPATH, '//main[1]/div/button[.=\"Load more\"]')))  # wait max timeout sec for loading\n",
    "                target_scroll_end = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                while scroll_pos <= target_scroll_end:\n",
    "                    driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")  # scroll page to bottom to load images\n",
    "                driver.execute_script(\"arguments[0].click();\", next_page_btn)  # click the load more button through ads\n",
    "                page += 1\n",
    "            except TimeoutException as e:\n",
    "                break\n",
    "    else:\n",
    "        while True:  # scrap all\n",
    "            try:\n",
    "                next_page_btn = WebDriverWait(driver, timeout).until(EC.presence_of_element_located((By.XPATH, '//main[1]/div/button[.=\"Load more\"]')))  # wait max timeout sec for loading\n",
    "                driver.execute_script(\"arguments[0].click();\", next_page_btn)  # click the load more button through ads\n",
    "                page += 1\n",
    "            except TimeoutException as e:\n",
    "                break\n",
    "    time.sleep(timeout)\n",
    "    print(f'All results loaded. Total: {page} pages.')\n",
    "    return BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "def parse_info(item_div, mode=1):\n",
    "    try:\n",
    "        a = item_div.div.find_all('a', recursive=False)\n",
    "        seller_divs = a[0].find_all('div', recursive=False)[1]\n",
    "        item_p = a[1].find_all('p', recursive=False)\n",
    "        img = a[1].div.find_all('div', recursive=False)[-1].img['src']\n",
    "        likes = item_div.find_all('div', recursive=False)[1].button.span.get_text()\n",
    "        if mode == 1:\n",
    "            return {'seller_name': seller_divs.p.get_text(),\n",
    "                    'seller_url': home+a[0]['href'],\n",
    "                    'item_name': a[1].find_all('div', recursive=False)[1].p.get_text(),\n",
    "                    'item_url': home+a[1]['href'],\n",
    "                    'img': img,\n",
    "                    'time_posted': seller_divs.div.p.get_text(),  # TODO: process into absolute datetime\n",
    "                    'condition': item_p[1].get_text(),\n",
    "                    'likes': likes,\n",
    "                    'price': re.findall(r\"\\d+\", item_p[0].get_text().replace(',', ''))[0]}  # 0 is discounted price, 1 is original price, if applicable\n",
    "        else:\n",
    "            return {'seller_name': seller_divs.p.get_text(),\n",
    "                    'seller_url': home+a[0]['href'],\n",
    "                    'item_name': item_p[0].get_text(),\n",
    "                    'item_url': home+a[1]['href'],\n",
    "                    'img': img,\n",
    "                    'time_posted': seller_divs.div.p.get_text(),  # TODO: process into absolute datetime\n",
    "                    'condition': item_p[3].get_text(),\n",
    "                    'likes': likes,\n",
    "                    'price': re.findall(r\"\\d+\", item_p[1].get_text().replace(',', ''))[0]}  # 0 is discounted price, 1 is original price, if applicable\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "home = 'https://sg.carousell.com'\n",
    "item = input('Enter item to scrape: ')\n",
    "page_limit = int(input('Up to how many pages to scrap? Each page is 23-25 listings, enter 0 to scrap all: '))\n",
    "extension = f'/search/{urllib.parse.quote(item)}'\n",
    "opts = Options()\n",
    "opts.add_experimental_option('prefs', {'intl.accept_languages': 'en,en_US'})\n",
    "caps = DesiredCapabilities().CHROME\n",
    "caps[\"pageLoadStrategy\"] = \"normal\"  #  Waits for full page load\n",
    "driver = webdriver.Chrome(desired_capabilities=caps, options=opts)\n",
    "# driver.minimize_window()\n",
    "print(f'Chrome Web Driver loaded. Version: {driver.capabilities[\"browserVersion\"]}\\n')  # use \"version\" on Linux\n",
    "parse_mode = 1  # Carousell have 2 formats of their item divs. See below comment for more info.\n",
    "tries = 1\n",
    "\n",
    "while tries < 5:  # retrying loop as the div class position is random\n",
    "    try:\n",
    "        print(f'Retrieving search results on {item}...')\n",
    "        search_results_soup = request_page(home+extension)\n",
    "        # TODO: Find concrete way to locate correct class name, current work around works 99% of times.\n",
    "        item_divs_class = ' '.join(search_results_soup.find('main').find('div').find('div').find('div')['class'])  # changes randomly but 99% of the time its the first div\n",
    "        print(f'Detected item_divs class: {item_divs_class}')\n",
    "        item_divs = search_results_soup.find('main').find('div').find('div').find_all('div', class_=item_divs_class, recursive=False)  # filter out ads divs\n",
    "        print(f'Found {len(item_divs)} listings. Parsing...')\n",
    "        items_list = [parse_info(item_div, parse_mode) for item_div in item_divs]\n",
    "        break\n",
    "    except AttributeError as e:  # no item_divs at all\n",
    "        print(e)\n",
    "        raise RuntimeError('The search has returned no result.')\n",
    "    except IndexError as e:\n",
    "        print(e)\n",
    "        print(f'Parsing attempt {tries} failed due to class name error using parse mode {parse_mode}. Retrying with parse mode 2...\\n')\n",
    "        tries += 1\n",
    "        parse_mode = 2\n",
    "        continue\n",
    "else:\n",
    "    raise RuntimeError('Parsing failed as it still faces IndexError after 5 tries.')\n",
    "\n",
    "driver.quit()\n",
    "print(f'Parse success using mode {parse_mode}! Sample item parsed:')\n",
    "pprint(items_list[0])\n",
    "df = pd.DataFrame(items_list)\n",
    "print(df.describe())\n",
    "print(df)\n",
    "df.to_csv(f'{item}.csv', index=False)\n",
    "print(f'Results saved to {item}.csv')\n",
    "\n",
    "'''\n",
    "Two parse modes only differs in item divs 2nd a\n",
    "Structure of Carousell HTML FORMAT 1 (parse_mode 1):\n",
    "body > find main > 1st div > 1st div > divs of items\n",
    "    in divs of items > parents of each item\n",
    "        parent > 1st div > 1st a is seller, 2nd a is item page\n",
    "            in 1st a: 2nd div > p is seller name, > div > p is time posted\n",
    "            in 2nd a: 2nd div > p is item name but with ... if too long, directly under 2nd a first p is price, 2nd p is condition\n",
    "        parent > 2nd div > button > span is number of likes\n",
    "total 24 or 25 results loaded once.\n",
    "\n",
    "Structure of Carousell HTML FORMAT 2 (parse_mode 2):\n",
    "body > find main > 1st div > 1st div > divs of items\n",
    "    in divs of items > parents of each item\n",
    "        parent > 1st div > 1st a is seller, 2nd a is item page\n",
    "            in 1st a: 2nd div > p is seller name, > div > p is time posted\n",
    "            in 2nd a: 1st p is FULl NAME, 2nd p is price, 3rd p is description, 4th p is condition\n",
    "        parent > 2nd div > button > span is number of likes\n",
    "total 24 or 25 results loaded once.\n",
    "\n",
    "body > find main > div > button to view more\n",
    "view more button loads on top of existing, so can prob spam view more then gather all items at once\n",
    "MAY NOT BE FIRST DIV! Temp workaround is to get class name of the correct item divs\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'start_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [40]\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     23\u001B[0m failed_items \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     24\u001B[0m time_taken \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m---> 25\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m \u001B[38;5;28mid\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[43mstart_id\u001B[49m, start_id\u001B[38;5;241m-\u001B[39mitems_to_scrap, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m     26\u001B[0m     start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m     27\u001B[0m     url \u001B[38;5;241m=\u001B[39m prefix \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'start_id' is not defined"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "\n",
    "def request_page(url):\n",
    "    \"\"\" Returns BeautifulSoup4 Objects (soup)\"\"\"\n",
    "    # driver.get(url)\n",
    "    r = requests.get(url)\n",
    "    return BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "# opts = Options()\n",
    "# opts.add_experimental_option('prefs', {'intl.accept_languages': 'en,en_US'})\n",
    "# caps = DesiredCapabilities().CHROME\n",
    "# caps[\"pageLoadStrategy\"] = \"normal\"  #  Waits for full page load\n",
    "# driver = webdriver.Chrome(desired_capabilities=caps, options=opts)\n",
    "# # driver.minimize_window()\n",
    "# print(f'Chrome Web Driver loaded. Version: {driver.capabilities[\"browserVersion\"]}\\n')  # use \"version\" on Linux\n",
    "\n",
    "items = {}\n",
    "prefix = 'https://www.carousell.sg/p/'\n",
    "# id = 1134572107\n",
    "failed_items = 0\n",
    "time_taken = 0\n",
    "for id in range(start_id, start_id-items_to_scrap, -1):\n",
    "    start = time.time()\n",
    "    url = prefix + str(id)\n",
    "    item_soup = request_page(url)\n",
    "    try:\n",
    "        item_soup = item_soup.body.find('div').find('div').find_all('div', recursive=False)[2]\n",
    "        top_section = item_soup.find('section').find('div').find('div').find('div')\n",
    "        title = item_soup.find_all('div', recursive=False)[1].find('div').find('div').find('div').find('p').get_text().strip()\n",
    "        likes = top_section.find_all('div', recursive=False)[1].find_all('button', recursive=False)[-1].find('p').get_text().split()[0]  # remove trailing 'likes'\n",
    "        img = top_section.find_all('div', recursive=False)[-1].find('div').find('img')['src']\n",
    "        item_details_soup = item_soup.find_all('div', recursive=False)[1].find('div').find('div').find_all('div', recursive=False)[1].find('section').find_all('div', recursive=False)[3]\n",
    "        # item_details_soup = [soup for soup in item_details_soup if soup.get_text() == 'Description'][0]  # position of div may change so filter based on text\n",
    "        item_details_divs = item_details_soup.find_all('div', recursive=False)\n",
    "        attribute_divs = item_details_divs[0].find('div').find_all('div', recursive=False)\n",
    "        attributes = {div.find_all('p', recursive=False)[0].get_text(): div.find_all('p', recursive=False)[1].get_text() for div in attribute_divs}\n",
    "        attributes.pop('Posted')  # remove time posted, also act as a check, if got error, then scraping algo likely failed\n",
    "        description = item_details_divs[1].get_text().replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ').strip()\n",
    "        items.update({id: {'title': title, 'url': url, 'img': img, 'attributes': attributes, 'desc': description, 'likes': likes}})\n",
    "    except Exception as e:  # means product already delisted, attribute error confirm is, rest is other reasons\n",
    "        failed_items += 1\n",
    "        print(url, e)\n",
    "    end = time.time()\n",
    "    time_taken += end - start\n",
    "\n",
    "print(f'Scraping completed. Successfully scraped {len(items)}, failed {failed_items}, total time taken: {time_taken}, average time taken: {round(time_taken / len(items_to_scrap), 5)}')\n",
    "\n",
    "with open('data.json', 'w') as f:\n",
    "    json.dump(items, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "count = 40  # 40 max\n",
    "query = 'rtx 3080ti'\n",
    "url = 'https://www.carousell.sg/api-service/search/cf/4.0/search/'\n",
    "product_url_prefix = 'https://www.carousell.sg/p/'\n",
    "header = {\"bestMatchEnabled\":'true',\"canChangeKeyword\":'true',\"count\":count,\"countryCode\":\"SG\",\"countryId\":\"1880251\",\"filters\":[],\"includeSuggestions\":'true',\"locale\":\"en\",\"prefill\":{},\"query\": query}\n",
    "r = requests.post(url, data=header)\n",
    "r = json.loads(r.text)\n",
    "total_results_in_db = r['data']['formattedTotal']  # string as it may include '10000+'\n",
    "r = r['data']['results']\n",
    "print('total results returned:', len(r))\n",
    "for i in r:\n",
    "    i = i['listingCard']\n",
    "    print(i['seller']['username'])\n",
    "    print(i['photoUrls'])\n",
    "    print(i['price'])\n",
    "    print(i['title'])\n",
    "    print(i['likesCount'])\n",
    "    link = product_url_prefix+i['id']\n",
    "    print(link)\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import shutil\n",
    "\n",
    "with open('data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "for i, v in data.items():\n",
    "    r = requests.get(v['img'], stream=True)\n",
    "    r.raw.decode_content = True\n",
    "    with open(f'imgs/{i}.jpg','wb') as f:\n",
    "        shutil.copyfileobj(r.raw, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}